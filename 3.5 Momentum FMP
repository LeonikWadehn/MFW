import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from pathlib import Path
from typing import Dict, List, Tuple
import statsmodels.api as sm    # for Newey–West HAC t‑statistic
from pandas.tseries.offsets import MonthEnd


#============================
#   User Parameters
#============================
input_data_path          = Path("1. Total Return.parquet")
riskfree_data_path       = Path("/Users/leonikwadehn/Documents/Uni/ISEG/MFW/Python/Try 8/RF.xlsx")

analysis_start           = "2006-12-31"
analysis_end             = "2024-12-31"

# How often to form the portfolios: 'Monthly', 'Quarterly', or 'Annual'
formation_frequency      = "Monthly"
# How to split assets into groups: 'Median', 'Quintiles', or '304030'
grouping_method          = "Median"
# How to weight returns in each group: 'Equal' or 'Value'
weighting_scheme         = "Value"

# Winsorization trims extreme values to reduce outlier effects
winsor_lower_q           = 0.0025
winsor_upper_q           = 0.9975

# Momentum‐specific parameters
momentum_lookback_period = 6   # how many months of past returns to cumulate
momentum_skip_months     = 1   # how many most‑recent months to skip

# Where to save output files
results_dir = Path("/Users/leonikwadehn/Documents/Uni/ISEG/MFW/Python/Try 8/Factor Outputs")
results_dir.mkdir(parents=True, exist_ok=True)


#============================
#   Helper Functions
#============================
def winsorize(series: pd.Series,
              lower_q: float = winsor_lower_q,
              upper_q: float = winsor_upper_q) -> pd.Series:
    """Clip a Series at its (lower_q, upper_q) quantiles."""
    lo = series.quantile(lower_q)
    hi = series.quantile(upper_q)
    return series.clip(lower=lo, upper=hi)


def compute_group_breakpoints(measure: pd.Series,
                              method: str = "median") -> Dict[str, float]:
    """
    For the chosen grouping_method, return the numeric thresholds
    defining the 'low' vs 'high' groups.
    """
    m = method.lower()
    if m == "median":
        cut = measure.median()
        return {"low": cut, "high": cut}
    elif m == "quintiles":
        return {"low": measure.quantile(0.2),
                "high": measure.quantile(0.8)}
    elif m in ("304030", "30/40/30"):
        return {"low": measure.quantile(0.3),
                "high": measure.quantile(0.7)}
    else:
        raise ValueError("grouping_method must be 'Median', 'Quintiles', or '304030'.")


def assign_groups(measure: pd.Series,
                  breakpoints: Dict[str, float],
                  method: str = "median") -> Tuple[List[str], List[str]]:
    """
    Split assets into 'low' and 'high' groups based on breakpoints.
    Returns two lists of asset IDs.
    """
    m = method.lower()
    if m == "median":
        low_ids  = measure[measure <  breakpoints["low"]].index.tolist()
        high_ids = measure[measure >  breakpoints["high"]].index.tolist()
    else:
        low_ids  = measure[measure <= breakpoints["low"]].index.tolist()
        high_ids = measure[measure >= breakpoints["high"]].index.tolist()
    return low_ids, high_ids


def construct_momentum_factor_series(
    data: pd.DataFrame,
    frequency: str = "Monthly",
    grouping_method: str = "Median",
    weighting: str = "Value",
    lookback: int = momentum_lookback_period,
    skip: int = momentum_skip_months
) -> pd.DataFrame:
    """
    Build the time series of momentum factor returns = 
      (High‑momentum portfolio return) – (Low‑momentum portfolio return).

    1) On each formation date (by freq), compute past cumulative return:
         R_cum = prod(1 + r_t) – 1
       over the window (dt – skip – lookback, dt – skip].
    2) Winsorize R_cum, split into low/high by grouping_method.
    3) For each subsequent period, apply out‑of‑sample lag.
    4) Compute equal‑ or value‑weighted returns of each group, then subtract.
    """
    print(f"→ START {frequency=}  {grouping_method=}  {weighting=}")
    freq = frequency.lower()
    # 1. Identify formation dates
    if freq == "monthly":
        formation_dates = sorted(data["Date"].unique())
    elif freq == "quarterly":
        formation_dates = (
            data.groupby(data["Date"].dt.to_period("Q"))["Date"]
                .max().sort_values().tolist()
        )
    elif freq == "annual":
        formation_dates = (
            data.groupby(data["Date"].dt.to_period("Y-JUN"))["Date"]
                .max().sort_values().tolist()
        )
    else:
        raise ValueError("frequency must be 'Monthly', 'Quarterly', or 'Annual'.")

    # 2. On each formation date, determine low/high Momentum groups
    formation_map: Dict[pd.Timestamp, Dict] = {}
    for form_date in formation_dates:
        dt = pd.to_datetime(form_date)
        subset = data.loc[
            data["Date"] == dt,
            ["Company", "Total Return", "Historical Market Cap"]
        ].copy()
        if subset.empty:
            continue

        # define window bounds on true month‑ends
        window_end   = dt - MonthEnd(skip)
        window_start = dt - MonthEnd(skip + lookback)

        # pull all returns in (window_start, window_end]
        window = data[
            (data["Date"] > window_start) &
            (data["Date"] <= window_end)
        ][["Company", "Date", "Total Return"]].copy()

        # require exactly `lookback` observations
        counts = window.groupby("Company")["Total Return"].count()
        good   = counts[counts == lookback].index
        window = window[window["Company"].isin(good)]

        # compute cumulative return in one vectorized step
        cumret = (
            window.sort_values(["Company", "Date"])
                  .groupby("Company")["Total Return"]
                  .apply(lambda r: np.prod(1 + r) - 1)
        )

        # merge cumret back to this formation‑date subset
        subset = subset.merge(
            cumret.rename("Momentum"),
            left_on="Company", right_index=True, how="inner"
        )
        if subset.empty:
            continue

        # winsorize & split into groups
        m_series    = subset.set_index("Company")["Momentum"]
        m_wins      = winsorize(m_series)
        bp          = compute_group_breakpoints(m_wins, grouping_method)
        low_ids, high_ids = assign_groups(m_wins, bp, grouping_method)

        if weighting.lower() == "value":
            mcap = subset.set_index("Company")["Historical Market Cap"]
            formation_map[dt] = {
                "low_ids":  low_ids,
                "high_ids": high_ids,
                "low_wts":  mcap.loc[low_ids],
                "high_wts": mcap.loc[high_ids]
            }
        else:
            formation_map[dt] = {"low_ids": low_ids, "high_ids": high_ids}

    # 3. For each period, find the most recent formation date < period, compute returns
    records = []
    for period in sorted(data["Date"].unique()):
        dt = pd.to_datetime(period)
        valid_forms = [d for d in formation_map if d < dt]
        if not valid_forms:
            continue
        use_form = max(valid_forms)
        assign   = formation_map[use_form]
        period_df = data[data["Date"] == dt]

        # Compute portfolio returns
        if weighting.lower() == "equal":
            low_ret  = period_df[period_df["Company"].isin(assign["low_ids"])]["Total Return"].mean()
            high_ret = period_df[period_df["Company"].isin(assign["high_ids"])]["Total Return"].mean()
        else:
            low_data  = period_df[period_df["Company"].isin(assign["low_ids"])]
            high_data = period_df[period_df["Company"].isin(assign["high_ids"])]
            if not low_data.empty:
                r_low = low_data.set_index("Company")["Total Return"]
                w_low = assign["low_wts"].loc[r_low.index]
                low_ret = np.average(r_low.reindex(w_low.index), weights=w_low)
            else:
                low_ret = np.nan
            if not high_data.empty:
                r_high = high_data.set_index("Company")["Total Return"]
                w_high = assign["high_wts"].loc[r_high.index]
                high_ret = np.average(r_high.reindex(w_high.index), weights=w_high)
            else:
                high_ret = np.nan

        # High‑minus‑low
        records.append((dt, high_ret - low_ret))

    factor_df = pd.DataFrame(records, columns=["Date", "FactorReturn"])
    factor_df.dropna(subset=["FactorReturn"], inplace=True)
    factor_df.reset_index(drop=True, inplace=True)

    # snap each period to month‑end
    factor_df["Date"] = pd.to_datetime(factor_df["Date"])
    factor_df["Date"] = (
        factor_df["Date"]
            .dt.to_period("M")
            .dt.to_timestamp(how="end")
    )
    return factor_df


def load_riskfree_rate(path: Path) -> pd.DataFrame:
    """
    Read monthly risk-free rates from Excel and snap each date to month‑end.
    """
    rf = pd.read_excel(path, header=None)
    rf.columns = ["Date", "RF"]
    rf["Date"] = (
        pd.to_datetime(rf["Date"], errors="coerce")
          .dt.to_period("M")
          .dt.to_timestamp(how="end")
    )
    rf.dropna(subset=["Date"], inplace=True)
    #turn annualized EURIBOR into per‑month return
    # Act/360: monthly return ≈ RF_ann * (days_in_month/360)
    rf["Days"] = rf["Date"].dt.daysinmonth
    rf["RF"] = rf["RF"] * rf["Days"] / 360
    return rf


def merge_with_riskfree(factor_df: pd.DataFrame,
                        rf_df: pd.DataFrame) -> pd.DataFrame:
    """
    Left‑join factor returns to RF compute excess return.
    """
    merged = factor_df.merge(rf_df, on="Date", how="left")
    merged["ExcessReturn"] = merged["FactorReturn"] - merged["RF"]
    return merged


def compute_statistics(factor: pd.Series, excess: pd.Series) -> Dict[str, str]:
    """
    Return summary stats: mean, p‑value, median, vol, skew, kurt, Sharpe, Jarque‑Bera, Newey‑West.
    """
    mean_val    = np.nanmean(factor)
    median_val  = np.nanmedian(factor)
    vol         = np.nanstd(factor, ddof=1)
    skewness    = stats.skew(factor, bias=False, nan_policy="omit")
    kurtosis    = stats.kurtosis(factor, fisher=True, bias=False, nan_policy="omit") 
    tstat, pval = stats.ttest_1samp(factor.dropna(), 0.0)
    excess_mean = np.nanmean(excess)
    excess_vol  = np.nanstd(excess, ddof=1)
    sharpe      = excess_mean / excess_vol if excess_vol not in (0, np.nan) else np.nan
    jb_stat, jb_p = stats.jarque_bera(factor.dropna())
    nw_res = sm.OLS(factor.dropna(), np.ones(len(factor.dropna()))) \
              .fit(cov_type="HAC", cov_kwds={"maxlags": 3})
    nw_t, nw_p = nw_res.tvalues.iloc[0], nw_res.pvalues.iloc[0]

    return {
        "Mean":          f"{mean_val:.4f}",
        "(p-value)":     f"({pval:.3f})",
        "Median":        f"{median_val:.4f}",
        "Volatility":    f"{vol:.4f}",
        "Skewness":      f"{skewness:.4f}",
        "Kurtosis":      f"{kurtosis:.4f}",
        "Sharpe ratio":  f"{sharpe:.4f}",
        "Jarque-Bera":   f"{jb_stat:.2f} (p={jb_p:.3f})",
        "NW t‑stat":     f"{nw_t:.2f} (p={nw_p:.3f})",
    }


def plot_period_returns(factor_df: pd.DataFrame) -> None:
    """Bar chart of each period’s factor return."""
    df = factor_df.sort_values("Date")
    plt.figure(figsize=(14, 6))
    plt.bar(df["Date"], df["FactorReturn"], width=20)
    plt.axhline(0, linewidth=1, color="gray")
    plt.xlabel("Date")
    plt.ylabel("Period Return")
    plt.title("Momentum Factor Returns Over Time")
    plt.xticks(rotation=45)
    plt.grid(axis="y", linestyle="--", alpha=0.5)
    plt.tight_layout()
    plt.show()


def plot_cumulative_return_series(factor_df: pd.DataFrame) -> None:
    """Line plot of cumulative $1 growth from factor returns."""
    series = factor_df.set_index("Date")["FactorReturn"].sort_index()
    cum    = (1 + series).cumprod()
    plt.figure(figsize=(14, 6))
    plt.plot(cum.index, cum.values, marker="o", linewidth=2, label="Cumulative")
    plt.axhline(1, linestyle="--", linewidth=1, alpha=0.7)
    plt.xlabel("Date")
    plt.ylabel("Value of $1")
    plt.title("Cumulative Momentum Factor Return")
    plt.legend()
    plt.grid(linestyle="--", alpha=0.5)
    plt.tight_layout()
    plt.show()


def display_statistics(stats: Dict[str, str]) -> None:
    """Print summary statistics in a neat table."""
    print("\n=== Momentum Factor Summary Statistics ===")
    for metric, value in stats.items():
        print(f"{metric:12s}: {value}")
    print("============================================\n")


#============================
#   MAIN EXECUTION
#============================
def main() -> None:
    try:
        print("1) Loading input data…")
        df = pd.read_parquet(input_data_path)
        df["Date"] = pd.to_datetime(df["Date"])

        # Keep only desired dates & measures
        mask_time = df["Date"].between(analysis_start, analysis_end)
        df = df[mask_time & df["Financial Measure"].isin([
            "Total Return", "Historical Market Cap"
        ])].copy()

        # Pivot so each row is one (Company, Date) with needed columns
        df_wide = (
            df.pivot_table(
                index=["Company", "Date"],
                columns="Financial Measure",
                values="Value"
            )
            .dropna(subset=["Total Return", "Historical Market Cap"])
            .reset_index()
        )

        print("2) Forming portfolios & computing factor returns…")
        mom_df = construct_momentum_factor_series(
            df_wide,
            frequency=formation_frequency,
            grouping_method=grouping_method,
            weighting=weighting_scheme,
            lookback=momentum_lookback_period,
            skip=momentum_skip_months
        )

        print("3) Loading & merging risk‑free rates…")
        rf_df  = load_riskfree_rate(riskfree_data_path)
        merged = merge_with_riskfree(mom_df, rf_df)

        stats = compute_statistics(merged["FactorReturn"], merged["ExcessReturn"])
        display_statistics(stats)

        # Save outputs
        merged["Date"] = merged["Date"].dt.date
        filename_base = f"Momentum_{formation_frequency}_{grouping_method}"
        merged.to_csv(results_dir / (filename_base + ".csv"), index=False)
        merged.to_parquet(results_dir / (filename_base + ".parquet"), index=False)
        print(f"Results saved under {results_dir}")

        # Draw charts
        plot_period_returns(merged)
        plot_cumulative_return_series(merged)

    except Exception as err:
        print(f"An error occurred: {err}")
        raise


if __name__ == "__main__":
    main()
